Step 1: Import necessary libraries: Numpy, Pandas, Keras, Scikit-learn, and Matplotlib for neural network modeling.
 import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns



Step 2: Load the dataset predictive_maintenance_data.csv and display the first 5 rows.
df=pd.read_csv("predictive_maintenance_data.csv")
print("the First 5 values of data is :\n")
df.head()



Step 3: Define features X and target y for the model.
X = df.drop('failure', axis=1).values # Features (sensor data)
y = df['failure'].values # Target (failure labels)


Step 4: Scale the features using StandardScaler.
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)



Step 5: Split the data into training, testing, and validation sets.
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15,
random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15,
random_state=42)



Step 6: Build a sequential neural network model with three layers (64, 32 neurons, and 1
output).
model = Sequential()
model.add(Dense(64, input_dim=X.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))



Step 7: Compile the model using adam optimizer and binary_crossentropy loss.
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


Step 8: Train the model on training data and evaluate it on test data. Predict results on test data.
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10,
batch_size=32)
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))


Step 9: Generate and visualize a confusion matrix for model performance.
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Failure',
'Failure'], yticklabels=['No Failure', 'Failure'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()



Step 10: Plot the learning curves for loss over the epochs.
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Learning Curve: Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()



Step 11: Plot the learning curves for accuracy over the epochs.
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Learning Curve: Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()




